# -*- coding: utf-8 -*-
"""Best and Worst Average Ratings of the Movielens Data Set Using Spark 2.0.0

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gaRGAJYlsWX0bUnSskiihqhIiSl6uLIl

# **Best and Worst Average Ratings of the Movielens Data Set Using Spark 2.0.0**
Alyssa April Dellow

P125528

# **Organisation of Notebook**

This Notebook consists of four major sections as follows:

1. **Section 1** - Installation of the PySpark package and mounting of Google Drive.
2. **Section 2** - A breakdown of the steps taken to analyse the MovieLens data set according to the checklist provided. In this section, comments are included to explain each line of code for better comprehension.
3. **Section 3** - A final combination of all the codes to get an overall view of the process.
4. **Section 4** - A simple explanation of how efficient and optimal Spark transformations and actions are used in this Notebook. An error handling technique utilised is also briefly mentioned.

# **Section 1**

## **PySpark Package**
[Apache Spark](https://spark.apache.org/) is defined as a  "multi-language engine for executing data engineering, data science and machine learning on single-node machines or clusters". It is a big data processing and analytics distributed computing system. [Spark 2.0.0](https://spark.apache.org/releases/spark-release-2-0-0.html) was released on 26 July 2016 with major updates on the API usability, SQL 2003 support, performance improvements, structured streaming, R UDF support and operational improvements.

<p align="justify">Before being able to conduct an analysis on the MovieLens data set using Spark 2.0.0 in Google Colab, the PySpark package needs to be installed first. PySpark is included in the official Spark releases and is now available in the Python Package Index (PyPI), which is a repository of software for the Python programming language. By installing the PySpark package, it offers us the required tools and APIs for interacting with Spark via the execution of familiar Python codes, making our tasks much more convenient. Once PySpark has been imported, Spark's ability to handle data can be utilised to the fullest in completing our MovieLens analysis.
"""

# Install PySpark package
!pip install pyspark

# Mount Google Drive in Google Colab.
# This enables files saved in Google Drive to be accessed directly from this
# Notebook.
from google.colab import drive
drive.mount('/content/drive')

"""# **Section 2**

<p align="justify">This section goes through each code step by step and lists out the parts where each point in the checklist is accomplished.

**Import all the necessary modules**
"""

# The entry point to programming Spark with the Dataset and DataFrame API.
from pyspark.sql import SparkSession

# Import the ROW method, which takes up the argument for creating Row Object.
from pyspark.sql import Row

# col: Returns a Column based on the given column name.
# avg: Aggregate function that returns the average of the values in a group.
# min: Aggregate function that returns the minimum value of the expression in
# a group.
from pyspark.sql.functions import col, avg, min

"""**Successfully loaded the MovieLens dataset into Spark**"""

# Define the parseInput function
def parseInput(line):
  # Separate the input into individual fields based on whitespace.
  fields = line.split()
  # Create a Row object based on the parsed data of movie id, rating and
  # timestamp from the u.data file later on. The columns are named movieID,
  # rating and timestamp.
  return Row(movieID = int(fields[1]), rating = float(fields[2]),
             timestamp = float(fields[3]))

# Create a SparkSession using the supplied name "Best&WorstMovieRatings".
# This allows us to communicate with Spark and execute various actions on the
# data using the Spark APIs.
# getOrCreate() will retrieve an existing SparkSession or create a new one
# if it does not exist.
spark = SparkSession.builder.appName("Best&WorstMovieRatings").getOrCreate()

# Read-in the u.item file into Google Colab.
# Select only the movie id and movie title columns, then rename them.
item_df = spark.read.csv("/content/drive/MyDrive/DM_Assignment3/u.item",
                           header=False, sep="|")
item_df = item_df.select(col("_c0").alias("movieID"),
                             col("_c1").alias("movieName"))

# Print the first 10 rows of item_df to the console.
# The argument "truncate = False" ensures that the whole string is printed
# to the console.
item_df.show(n = 10, truncate = False)

# Read the raw data called u.data.
# Reads the text file from the path provided and returns a Resilient
# Distributed Dataset (RDD).
lines = spark.sparkContext.\
        textFile("/content/drive/MyDrive/DM_Assignment3/u.data")

# Convert the data into a RDD of Row objects with (movie id, rating and
# timestamp)
u_data = lines.map(parseInput)

# Convert the RDD of Row objects into a DataFrame and cache it
data_df = spark.createDataFrame(u_data).cache()

# Print the first 10 rows of data_df to the console
data_df.show(n = 10)

"""**Filtered out the movies with less than and equal to 100 ratings**"""

# Perform filtering to keep only movies with more than 100 ratings by first
# grouping the data in data_df by their movieID and then counting the
# number of rows for each group.
more_100_ratings = data_df.groupBy("movieID").count().filter("count > 100")

# Print the first 10 rows of more_100_ratings to the console
more_100_ratings.show(n = 10)

# This reflects the subset of movies with a significant number of ratings,
# which could somewhat be an indicator of popularity.

"""**Included only the oldest timestamp**

**Calculated the average rating for each movie**
"""

# Carries out an inner join between more_100_ratings and data_df based on
# the movieID column. For keys that don't match, the rows are dropped. Hence,
# only movies with more than 100 ratings are in filtered_movies.
filtered_movies = more_100_ratings.join(data_df,
                                           more_100_ratings.\
                                           movieID == data_df.movieID)

# Drops one of the movieID columns since they are redundant, and also drop count
filtered_movies = filtered_movies.drop(more_100_ratings.movieID, "count")

# Print the first 10 rows of filtered_movies to the console
filtered_movies.show(n = 10)

# Carries out an inner join between item_df and filtered_movies based on
# the movieID column.
filtered_movies = item_df.join(filtered_movies,
                                    item_df.movieID == filtered_movies.\
                                    movieID)

# Drops one of the movieID columns since they are redundant
filtered_movies = filtered_movies.drop(item_df.movieID)

# Print the first 10 rows of filtered_movies to the console
filtered_movies.show(n = 10, truncate = False)

# Group the data in data_df by their movieID and perform an
# aggregation operation on each group. Find the minimum
# value of timestamp within each group, which represents the oldest timestamp.
# Then rename the column as oldest_timestamp.
oldest_timestamp = data_df.groupBy("movieID").\
                   agg(min("timestamp").alias('oldest_timestamp'))

# Print the first 10 rows of oldest_timestamp to the console.
oldest_timestamp.show(n = 10)

# Carries out an inner join between oldest_timestamp and filtered_movies
# based on the movieID column. This way, rows without the oldest timestamp
# will be dropped.
filtered_movies = oldest_timestamp.\
                     join(filtered_movies, oldest_timestamp.\
                          movieID == filtered_movies.movieID)

# Drops one of the movieID columns and the timestamp column.
filtered_movies = filtered_movies.drop(oldest_timestamp.movieID,
                     filtered_movies.timestamp)

# Print the first 10 rows of filtered_movies to the console.
filtered_movies.show(n = 10, truncate = False)

# Group the data in filtered_movies based on the movieID, movieName and
# oldest_timestamp columns. Perform the aggregation operation on each group
# before computing the average value of rating for each group. This
# represents the average rating for a particular movie. After that, rename
# the column as average_rating.
avg_ratings_df = filtered_movies.groupBy(filtered_movies.movieID,
                                            item_df.movieName,
                                            oldest_timestamp.oldest_timestamp).\
                                            agg(avg("rating").\
                                            alias("average_rating"))

# Print the first 10 rows of avg_ratings_df to the console
avg_ratings_df.show(n = 10, truncate = False)

"""**Sorted the movies based on the average rating (in descending order)**"""

# Orders avg_ratings_df in descending order based on the average_rating values.
# This allows us to view movies with the highest average ratings first.
avg_ratings_df = avg_ratings_df.orderBy("average_rating", ascending = False)

# Print the first 10 rows of avg_ratings_df to the console
avg_ratings_df.show(n = 10, truncate = False)

"""**Selected the top 25 movies with the highest average rating**

**Selected the top 25 movies with the lowest average rating**
"""

# Since avg_ratings_df is already in descending order of average_rating, we
# limit avg_ratings to contain only the first 25 rows. This gives us the top
# 25 best movies based on average ratings.
best_25_average_ratings = avg_ratings_df.limit(25)

# To obtain the top 25 movies with the lowest average ratings,
# order avg_ratings_df in ascending order of average_rating, in order to display
# movies with the lowest average ratings first. Limit to 25 results.
worst_25_average_ratings = avg_ratings_df.orderBy("average_rating").limit(25)

# Print the first 25 rows of best_25_average_ratings to the console. This
# represents the top 25 movies with the best average ratings.
best_25_average_ratings.show(n = 25, truncate = False)

# Print the first 25 rows of worst_25_average_ratings to the console. This
# represents the top 25 movies with the worst average ratings.
worst_25_average_ratings.show(n = 25, truncate = False)

"""**Ordered the output results on the console by oldest timestamp**

**Displayed the results on the console**
"""

# The top 25 movies with the best average ratings are then ordered in
# ascending order of oldest_timestamp.
best_avg_rate_tmstmp = best_25_average_ratings.orderBy("oldest_timestamp")

# Rename the average_rating column as best_average_ratings
best_avg_rate_tmstmp = best_avg_rate_tmstmp.\
                          withColumnRenamed("average_rating",
                                            "best_average_ratings")

# The top 25 movies with the worst average ratings are ordered in
# ascending order of oldest_timestamp.
worst_avg_rate_tmstmp = worst_25_average_ratings.orderBy("oldest_timestamp")

# Rename the average_rating column as worst_average_ratings
worst_avg_rate_tmstmp = worst_avg_rate_tmstmp.\
                        withColumnRenamed("average_rating",
                                          "worst_average_ratings")

# Give an indication of what the table below is about
print("Top 25 Movies with the Best Average Ratings Sorted by " +
"Oldest Timestamp")
# Print the first 25 rows of best_avg_rate_tmstmp to the console.
best_avg_rate_tmstmp.show(n = 25, truncate = False)

# Give an indication of what the table below is about
print("Top 25 Movies with the Worst Average Ratings Sorted by " +
"Oldest Timestamp")
# Print the first 25 rows of worst_avg_rate_tmstmp to the console.
worst_avg_rate_tmstmp.show(n = 25, truncate = False)

"""**Saved the output of the results**

<p align="justify">The codes are inserted as comments since it will be run again in Section 3. This is to avoid redundant saving of files and errors.
"""

# Write best_avg_rate_tmstmp to a csv file called "best_average_ratings.csv" in
# the path specified.

#best_avg_rate_tmstmp.write.\
#csv("/content/drive/MyDrive/DM_Assignment3/best_average_ratings.csv",
#    header=True)

# Write worst_avg_rate_tmstmp to a csv file called "worst_average_ratings.csv"
# in the path specified.

#worst_avg_rate_tmstmp.write.\
#csv("/content/drive/MyDrive/DM_Assignment3/worst_average_ratings.csv",
#    header=True)

"""**Stop the underlying SparkContext**"""

# Stop the underlying SparkContext and release the resources associated with it.
# This function is new in Spark 2.0.0
spark.stop()

"""# **Section 3**

<p align="justify">This section provides an overall view of the codes utilised to get to the end results of the top 25 best and worst movies based on average ratings (with more than 100 ratings), ordered by oldest timestamp. The printing of data frames except for the two end result tables are excluded from this section.

Note: (`if __name__ == "__main__":`) is added in this section as an error handling technique that will be explained in Section 4.
"""

from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.functions import col, avg, min

def parseInput(line):
  fields = line.split()
  return Row(movieID = int(fields[1]), rating = float(fields[2]),
             timestamp = float(fields[3]))

if __name__ == "__main__":

  spark = SparkSession.builder.appName("Best&WorstMovieRatings").getOrCreate()

  item_df = spark.read.csv("/content/drive/MyDrive/DM_Assignment3/u.item",
                           header=False, sep="|")
  item_df = item_df.select(col("_c0").alias("movieID"),
                           col("_c1").alias("movieName"))

  lines = spark.sparkContext.\
  textFile("/content/drive/MyDrive/DM_Assignment3/u.data")

  u_data = lines.map(parseInput)

  data_df = spark.createDataFrame(u_data).cache()

  more_100_ratings = data_df.groupBy("movieID").count().filter("count > 100")

  filtered_movies = more_100_ratings.join(data_df, more_100_ratings.\
                                          movieID == data_df.movieID)
  filtered_movies = filtered_movies.drop(more_100_ratings.movieID)

  filtered_movies = item_df.join(filtered_movies, item_df.\
                                 movieID == filtered_movies.movieID)
  filtered_movies = filtered_movies.drop(item_df.movieID)

  oldest_timestamp = data_df.groupBy("movieID").agg(min("timestamp").\
                                                    alias('oldest_timestamp'))

  filtered_movies = oldest_timestamp.join(filtered_movies, oldest_timestamp.\
                                          movieID == filtered_movies.movieID)
  filtered_movies = filtered_movies.drop(oldest_timestamp.movieID,
                                         filtered_movies.timestamp)

  avg_ratings_df = filtered_movies.groupBy(filtered_movies.movieID,
                                           item_df.movieName,
                                           oldest_timestamp.oldest_timestamp).\
                                           agg(avg("rating").\
                                           alias("average_rating"))

  avg_ratings_df = avg_ratings_df.orderBy("average_rating", ascending = False)

  best_25_average_ratings = avg_ratings_df.limit(25)
  worst_25_average_ratings = avg_ratings_df.orderBy("average_rating").limit(25)

  best_avg_rate_tmstmp = best_25_average_ratings.orderBy("oldest_timestamp")
  best_avg_rate_tmstmp = best_avg_rate_tmstmp.\
                          withColumnRenamed("average_rating",
                                            "best_average_ratings")

  worst_avg_rate_tmstmp = worst_25_average_ratings.orderBy("oldest_timestamp")
  worst_avg_rate_tmstmp = worst_avg_rate_tmstmp.\
                        withColumnRenamed("average_rating",
                                          "worst_average_ratings")

  best_avg_rate_tmstmp.write.\
  csv("/content/drive/MyDrive/DM_Assignment3/best_average_ratings.csv",
      header=True)

  worst_avg_rate_tmstmp.write.\
  csv("/content/drive/MyDrive/DM_Assignment3/worst_average_ratings.csv",
      header=True)

  print("Top 25 Movies with the Best Average Ratings Sorted by " +
  "Oldest Timestamp")
  best_avg_rate_tmstmp.show(n = 25, truncate = False)

  print("Top 25 Movies with the Worst Average Ratings Sorted by " +
  "Oldest Timestamp")
  worst_avg_rate_tmstmp.show(n = 25, truncate = False)

  spark.stop()

"""# **Section 4**

**Used efficient and optimal Spark transformations and actions**

<p align="justify">By implementing efficient and optimal Spark transformations and actions as listed down below, the codes aim to process the data in Spark with improved performance.

1. **RDD of Row objects and Data Frame**: In the codes, an RDD was created, where it is a way of storing key value information/general information in an object on the cluster. It is able to handle failure in a resilient manner. Besides that, it is compatible with various APIs and libraries, making it beneficial for heavy data processing tasks. The RDD can be easily combined with data frame based activities by converting it into a data frame. It can also be converted into an RDD of Row objects, which are flexible and dynamic in representing data. Without the need for a specific schema, Row objects can convert between RDDs and DataFrames without any issues. Data frames are more efficient and thus building data frames on top of the RDD of Row objects can provide more efficient query execution and optimisation.
2. **Caching**: The *cache* function is then used to cache the data frame from earlier. By keeping the data frame in memory and preventing needless recomputation when the data is reused, caching enhances the performance of frequently accessed data.
3. **Column pruning**: By using the *drop* function, redundant and irrelavent columns are removed from being processed further. Even the "count" column is dropped right after it is no longer needed. The *count* function is a Spark action that should be removed when not required to avoid unnecessary usage of CPU cycles and other resources.
4. **Filtering**: The *filter* transformation is applied at an early stage, directly to the data frame, which reduces the data to be processed and analysed in the subsequent codes.
5. **Selecting relevant columns**: In reading the u.item data set, the *select* function is used to only retrieve relevant columns (movie ID and movie title) based on our query. This is more optimal than simply reading-in all the columns available.
6. **Group by**: The *groupBy* function aids in aggregating data based on the specified columns. This way, any calculations or computations can be done on each group more efficiently.
7. **Stopping the underlying SparkContext**: The *spark.stop* function is used to stop the SparkSession and release any resources held by Spark to ensure  appropriate resource deallocation and cleanup. If the SparkSession is not shut down, its cluster resources cannot be assigned to other tasks.

**Used appropriate error handling techniques**

The error handling technique utilised in the codes mainly comes from (`if __name__ == "__main__":`). Codes inside this if statement are only executed when the program is run directly by the Python interpreter. We can dictate what goes into this if statement. In the case of being run directly by the Python interpreter, `__name__`, which is a unique variable in Python, will be set to `__main__`. On the other hand, the code will not be executed if it is imported as a module. As a matter of fact, `__name__` will be set as the name of the module, thus rendering the script unable to be executed. We can then avoid unwanted executions. It is a mechanism to regulate how a script runs based on whether it is imported as a module or ran directly.
"""